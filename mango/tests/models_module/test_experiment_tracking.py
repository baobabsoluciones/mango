import os
import pickle
import shutil
from unittest import TestCase

import numpy as np
import pandas as pd
from catboost import CatBoostClassifier, CatBoostRegressor
from lightgbm import LGBMClassifier, LGBMRegressor
from pandas.testing import assert_frame_equal, assert_series_equal
from sklearn.compose import ColumnTransformer
from sklearn.datasets import make_classification, make_regression
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

from mango.models.experiment_tracking import (
    export_model,
    MLExperiment,
    MLTracker,
)
from mango.models.enums import ProblemType, ModelLibrary


class InvalidModel:
    """
    Dummy class to test errors
    """

    pass


class TestExperimentTracking(TestCase):
    """
    Tes suite for the experiment tracking module inside models.
    """

    folder_name = "test_experiment_tracking"

    @classmethod
    def setUpClass(cls):
        """
        Create data for the tests and needed folders.
        """

        # Classification
        X_clf, y_clf = make_classification(
            n_samples=1000, n_features=10, random_state=42, n_classes=3, n_informative=5
        )
        X_clf = pd.DataFrame(X_clf, columns=[f"feature_{i}" for i in range(10)])
        y_clf = pd.Series(y_clf, name="target")

        # Shuffle
        X_clf = X_clf.sample(frac=1, random_state=42)
        y_clf = y_clf[X_clf.index]

        # Split
        cls.X_train_clf = X_clf[: int(len(X_clf) * 0.8)].reset_index(drop=True)
        cls.y_train_clf = y_clf[: int(len(y_clf) * 0.8)].reset_index(drop=True)
        cls.X_test_clf = X_clf[int(len(X_clf) * 0.8) :].reset_index(drop=True)
        cls.y_test_clf = y_clf[int(len(y_clf) * 0.8) :].reset_index(drop=True)

        # Regression
        X_reg, y_reg = make_regression(n_samples=1000, n_features=10, random_state=42)
        X_reg = pd.DataFrame(X_reg, columns=[f"feature_{i}" for i in range(10)])
        y_reg = pd.Series(y_reg, name="target")

        # Shuffle
        X_reg = X_reg.sample(frac=1, random_state=42)
        y_reg = y_reg[X_reg.index]

        # Split
        cls.X_train_reg = X_reg[: int(len(X_reg) * 0.8)].reset_index(drop=True)
        cls.y_train_reg = y_reg[: int(len(y_reg) * 0.8)].reset_index(drop=True)
        cls.X_test_reg = X_reg[int(len(X_reg) * 0.8) :].reset_index(drop=True)
        cls.y_test_reg = y_reg[int(len(y_reg) * 0.8) :].reset_index(drop=True)

        # Binary Classification
        X_bin_clf, y_bin_clf = make_classification(
            n_samples=1000, n_features=10, random_state=42, n_classes=2, n_informative=5
        )
        X_bin_clf = pd.DataFrame(X_bin_clf, columns=[f"feature_{i}" for i in range(10)])
        y_bin_clf = pd.Series(y_bin_clf, name="target")

        # Shuffle
        X_bin_clf = X_bin_clf.sample(frac=1, random_state=42)
        y_bin_clf = y_bin_clf[X_bin_clf.index]

        # Split
        cls.X_train_bin_clf = X_bin_clf[: int(len(X_bin_clf) * 0.8)].reset_index(
            drop=True
        )
        cls.y_train_bin_clf = y_bin_clf[: int(len(y_bin_clf) * 0.8)].reset_index(
            drop=True
        )
        cls.X_test_bin_clf = X_bin_clf[int(len(X_bin_clf) * 0.8) :].reset_index(
            drop=True
        )
        cls.y_test_bin_clf = y_bin_clf[int(len(y_bin_clf) * 0.8) :].reset_index(
            drop=True
        )

        # Expected values for roc curve
        cls.expected_tpr_logistic = [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9696969696969697,
            0.9696969696969697,
            0.9696969696969697,
            0.9595959595959596,
            0.9494949494949495,
            0.9393939393939394,
            0.9393939393939394,
            0.9393939393939394,
            0.9191919191919192,
            0.9090909090909091,
            0.9090909090909091,
            0.9090909090909091,
            0.898989898989899,
            0.8888888888888888,
            0.8787878787878788,
            0.8787878787878788,
            0.8787878787878788,
            0.8686868686868687,
            0.8686868686868687,
            0.8686868686868687,
            0.8686868686868687,
            0.8484848484848485,
            0.8282828282828283,
            0.8080808080808081,
            0.797979797979798,
            0.797979797979798,
            0.7878787878787878,
            0.7878787878787878,
            0.7878787878787878,
            0.7878787878787878,
            0.7878787878787878,
            0.7878787878787878,
            0.7575757575757576,
            0.7373737373737373,
            0.7272727272727273,
            0.7272727272727273,
            0.7272727272727273,
            0.7171717171717171,
            0.6868686868686869,
            0.6868686868686869,
            0.6565656565656566,
            0.6565656565656566,
            0.6464646464646465,
            0.6363636363636364,
            0.6262626262626263,
            0.5959595959595959,
            0.5757575757575758,
            0.5656565656565656,
            0.5555555555555556,
            0.5454545454545454,
            0.5353535353535354,
            0.5252525252525253,
            0.5252525252525253,
            0.5151515151515151,
            0.47474747474747475,
            0.45454545454545453,
            0.4444444444444444,
            0.40404040404040403,
            0.37373737373737376,
            0.3333333333333333,
            0.3333333333333333,
            0.3333333333333333,
            0.29292929292929293,
            0.26262626262626265,
            0.25252525252525254,
            0.21212121212121213,
            0.1919191919191919,
            0.1717171717171717,
            0.1414141414141414,
            0.12121212121212122,
            0.09090909090909091,
            0.0707070707070707,
            0.04040404040404041,
            0.0,
        ]
        cls.expected_fpr_logistic = [
            0.9504950495049505,
            0.8811881188118812,
            0.8415841584158416,
            0.7821782178217822,
            0.7425742574257426,
            0.7128712871287128,
            0.6831683168316832,
            0.6831683168316832,
            0.6435643564356436,
            0.6237623762376238,
            0.594059405940594,
            0.594059405940594,
            0.5841584158415841,
            0.5643564356435643,
            0.5445544554455446,
            0.5445544554455446,
            0.5148514851485149,
            0.504950495049505,
            0.4752475247524752,
            0.46534653465346537,
            0.45544554455445546,
            0.45544554455445546,
            0.43564356435643564,
            0.42574257425742573,
            0.39603960396039606,
            0.37623762376237624,
            0.36633663366336633,
            0.36633663366336633,
            0.3564356435643564,
            0.3465346534653465,
            0.33663366336633666,
            0.33663366336633666,
            0.32673267326732675,
            0.32673267326732675,
            0.31683168316831684,
            0.297029702970297,
            0.2871287128712871,
            0.27722772277227725,
            0.27722772277227725,
            0.26732673267326734,
            0.25742574257425743,
            0.25742574257425743,
            0.25742574257425743,
            0.2376237623762376,
            0.22772277227722773,
            0.22772277227722773,
            0.22772277227722773,
            0.21782178217821782,
            0.2079207920792079,
            0.2079207920792079,
            0.2079207920792079,
            0.2079207920792079,
            0.2079207920792079,
            0.19801980198019803,
            0.19801980198019803,
            0.19801980198019803,
            0.19801980198019803,
            0.1782178217821782,
            0.1782178217821782,
            0.16831683168316833,
            0.15841584158415842,
            0.15841584158415842,
            0.15841584158415842,
            0.1485148514851485,
            0.1485148514851485,
            0.1485148514851485,
            0.1485148514851485,
            0.1485148514851485,
            0.13861386138613863,
            0.13861386138613863,
            0.13861386138613863,
            0.13861386138613863,
            0.1188118811881188,
            0.10891089108910891,
            0.10891089108910891,
            0.09900990099009901,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.07920792079207921,
            0.06930693069306931,
            0.0594059405940594,
            0.04950495049504951,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.0297029702970297,
            0.0297029702970297,
            0.019801980198019802,
            0.019801980198019802,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
        ]
        cls.expected_precision_logistic = [
            0.5077,
            0.5266,
            0.538,
            0.5562,
            0.569,
            0.5789,
            0.5893,
            0.5893,
            0.6037,
            0.6087,
            0.6203,
            0.6203,
            0.6242,
            0.6323,
            0.6405,
            0.6405,
            0.6533,
            0.6577,
            0.6712,
            0.6759,
            0.6806,
            0.6806,
            0.6901,
            0.6929,
            0.708,
            0.7185,
            0.7218,
            0.7218,
            0.7273,
            0.7308,
            0.7344,
            0.7323,
            0.7381,
            0.7381,
            0.7398,
            0.75,
            0.7563,
            0.7627,
            0.7607,
            0.7652,
            0.7699,
            0.7699,
            0.7699,
            0.7818,
            0.789,
            0.789,
            0.789,
            0.7925,
            0.7961,
            0.7921,
            0.79,
            0.79,
            0.7879,
            0.7959,
            0.7959,
            0.7959,
            0.7959,
            0.8125,
            0.8065,
            0.8111,
            0.8182,
            0.8182,
            0.8182,
            0.8256,
            0.8193,
            0.8193,
            0.8125,
            0.8125,
            0.8205,
            0.8182,
            0.8158,
            0.8082,
            0.8261,
            0.8358,
            0.8333,
            0.8438,
            0.8548,
            0.8525,
            0.8525,
            0.85,
            0.8393,
            0.8333,
            0.8302,
            0.8333,
            0.8409,
            0.8462,
            0.8684,
            0.8919,
            0.8788,
            0.8667,
            0.8929,
            0.875,
            0.9048,
            0.8947,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            0,
        ]
        cls.expected_recall_logistic = [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9798,
            0.9798,
            0.9798,
            0.9697,
            0.9697,
            0.9697,
            0.9596,
            0.9495,
            0.9394,
            0.9394,
            0.9394,
            0.9192,
            0.9091,
            0.9091,
            0.9091,
            0.899,
            0.8889,
            0.8788,
            0.8788,
            0.8788,
            0.8687,
            0.8687,
            0.8687,
            0.8687,
            0.8485,
            0.8283,
            0.8081,
            0.798,
            0.798,
            0.7879,
            0.7879,
            0.7879,
            0.7879,
            0.7879,
            0.7879,
            0.7576,
            0.7374,
            0.7273,
            0.7273,
            0.7273,
            0.7172,
            0.6869,
            0.6869,
            0.6566,
            0.6566,
            0.6465,
            0.6364,
            0.6263,
            0.596,
            0.5758,
            0.5657,
            0.5556,
            0.5455,
            0.5354,
            0.5253,
            0.5253,
            0.5152,
            0.4747,
            0.4545,
            0.4444,
            0.404,
            0.3737,
            0.3333,
            0.3333,
            0.3333,
            0.2929,
            0.2626,
            0.2525,
            0.2121,
            0.1919,
            0.1717,
            0.1414,
            0.1212,
            0.0909,
            0.0707,
            0.0404,
            0.0,
        ]
        cls.expected_threshold_roc_curve_logistic = 0.47
        cls.expected_threshold_pr_logistic = 0.47

        cls.expected_tpr_catboost = [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9696969696969697,
            0.9696969696969697,
            0.9696969696969697,
            0.9696969696969697,
            0.9696969696969697,
            0.9696969696969697,
            0.9696969696969697,
            0.9595959595959596,
            0.9595959595959596,
            0.9595959595959596,
            0.9494949494949495,
            0.9494949494949495,
            0.9393939393939394,
            0.9292929292929293,
            0.9292929292929293,
            0.9292929292929293,
            0.9292929292929293,
            0.9191919191919192,
            0.9191919191919192,
            0.9090909090909091,
            0.8888888888888888,
            0.8787878787878788,
            0.8686868686868687,
            0.8383838383838383,
            0.8080808080808081,
            0.7777777777777778,
            0.7676767676767676,
            0.7373737373737373,
            0.7272727272727273,
            0.696969696969697,
            0.696969696969697,
            0.6666666666666666,
            0.6464646464646465,
            0.6363636363636364,
            0.6161616161616161,
            0.5858585858585859,
            0.494949494949495,
            0.46464646464646464,
            0.3939393939393939,
            0.35353535353535354,
            0.31313131313131315,
            0.2828282828282828,
            0.1919191919191919,
            0.0707070707070707,
            0.010101010101010102,
            0.0,
            0.0,
        ]
        cls.expected_fpr_catboost = [
            1.0,
            1.0,
            0.9306930693069307,
            0.8415841584158416,
            0.7128712871287128,
            0.594059405940594,
            0.5445544554455446,
            0.48514851485148514,
            0.44554455445544555,
            0.40594059405940597,
            0.38613861386138615,
            0.3465346534653465,
            0.33663366336633666,
            0.297029702970297,
            0.297029702970297,
            0.25742574257425743,
            0.2376237623762376,
            0.21782178217821782,
            0.21782178217821782,
            0.19801980198019803,
            0.18811881188118812,
            0.18811881188118812,
            0.1782178217821782,
            0.16831683168316833,
            0.16831683168316833,
            0.16831683168316833,
            0.16831683168316833,
            0.16831683168316833,
            0.1485148514851485,
            0.1485148514851485,
            0.1485148514851485,
            0.1485148514851485,
            0.1485148514851485,
            0.1485148514851485,
            0.13861386138613863,
            0.13861386138613863,
            0.13861386138613863,
            0.12871287128712872,
            0.12871287128712872,
            0.1188118811881188,
            0.10891089108910891,
            0.10891089108910891,
            0.09900990099009901,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.07920792079207921,
            0.07920792079207921,
            0.07920792079207921,
            0.06930693069306931,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.04950495049504951,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.0297029702970297,
            0.0297029702970297,
            0.0297029702970297,
            0.0297029702970297,
            0.0297029702970297,
            0.0297029702970297,
            0.0297029702970297,
            0.019801980198019802,
            0.009900990099009901,
            0.009900990099009901,
            0.009900990099009901,
            0.009900990099009901,
            0.009900990099009901,
            0.009900990099009901,
            0.009900990099009901,
            0.009900990099009901,
            0.009900990099009901,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
        ]
        cls.expected_precision_catboost = [
            0.495,
            0.495,
            0.513,
            0.538,
            0.5789,
            0.6226,
            0.6429,
            0.6689,
            0.6875,
            0.7071,
            0.7174,
            0.7388,
            0.7444,
            0.7674,
            0.7674,
            0.792,
            0.8049,
            0.8182,
            0.8182,
            0.8319,
            0.839,
            0.839,
            0.8462,
            0.8534,
            0.8534,
            0.8522,
            0.8522,
            0.8522,
            0.8673,
            0.8673,
            0.8673,
            0.8673,
            0.8673,
            0.8673,
            0.875,
            0.875,
            0.875,
            0.8829,
            0.8829,
            0.8909,
            0.8991,
            0.8991,
            0.9074,
            0.9159,
            0.9159,
            0.9159,
            0.9159,
            0.9238,
            0.9238,
            0.9238,
            0.9327,
            0.9417,
            0.9417,
            0.9412,
            0.9412,
            0.9412,
            0.9412,
            0.9412,
            0.9412,
            0.9412,
            0.9406,
            0.9406,
            0.9406,
            0.94,
            0.94,
            0.9394,
            0.9388,
            0.9388,
            0.9388,
            0.9388,
            0.9381,
            0.9381,
            0.9375,
            0.9462,
            0.956,
            0.9556,
            0.954,
            0.9524,
            0.9625,
            0.962,
            0.9605,
            0.96,
            0.9583,
            0.9583,
            0.9565,
            0.9697,
            0.9844,
            0.9839,
            0.9831,
            0.98,
            0.9787,
            0.975,
            0.9722,
            0.9688,
            0.9655,
            1.0,
            1.0,
            1.0,
            0,
            0,
        ]
        cls.expected_recall_catboost = [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9798,
            0.9798,
            0.9798,
            0.9798,
            0.9798,
            0.9798,
            0.9697,
            0.9697,
            0.9697,
            0.9697,
            0.9697,
            0.9697,
            0.9697,
            0.9596,
            0.9596,
            0.9596,
            0.9495,
            0.9495,
            0.9394,
            0.9293,
            0.9293,
            0.9293,
            0.9293,
            0.9192,
            0.9192,
            0.9091,
            0.8889,
            0.8788,
            0.8687,
            0.8384,
            0.8081,
            0.7778,
            0.7677,
            0.7374,
            0.7273,
            0.697,
            0.697,
            0.6667,
            0.6465,
            0.6364,
            0.6162,
            0.5859,
            0.4949,
            0.4646,
            0.3939,
            0.3535,
            0.3131,
            0.2828,
            0.1919,
            0.0707,
            0.0101,
            0.0,
            0.0,
        ]
        cls.expected_threshold_pr_catboost = 0.53
        cls.expected_threshold_roc_curve_catboost = 0.53

        cls.expected_tpr_lightgbm = [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.98989898989899,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9797979797979798,
            0.9696969696969697,
            0.9595959595959596,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9494949494949495,
            0.9393939393939394,
            0.9393939393939394,
            0.9393939393939394,
            0.9393939393939394,
            0.9292929292929293,
            0.9292929292929293,
            0.9292929292929293,
            0.9191919191919192,
            0.9191919191919192,
            0.9191919191919192,
            0.9191919191919192,
            0.9191919191919192,
            0.9090909090909091,
            0.9090909090909091,
            0.9090909090909091,
            0.9090909090909091,
            0.9090909090909091,
            0.898989898989899,
            0.8888888888888888,
            0.8888888888888888,
            0.8787878787878788,
            0.8787878787878788,
            0.8787878787878788,
            0.8787878787878788,
            0.8686868686868687,
            0.8686868686868687,
            0.8686868686868687,
            0.8484848484848485,
            0.8383838383838383,
            0.7878787878787878,
            0.7171717171717171,
            0.0,
        ]
        cls.expected_fpr_lightgbm = [
            0.2376237623762376,
            0.21782178217821782,
            0.1782178217821782,
            0.16831683168316833,
            0.1485148514851485,
            0.13861386138613863,
            0.13861386138613863,
            0.12871287128712872,
            0.1188118811881188,
            0.1188118811881188,
            0.09900990099009901,
            0.09900990099009901,
            0.09900990099009901,
            0.09900990099009901,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.0891089108910891,
            0.07920792079207921,
            0.07920792079207921,
            0.06930693069306931,
            0.06930693069306931,
            0.06930693069306931,
            0.06930693069306931,
            0.06930693069306931,
            0.06930693069306931,
            0.06930693069306931,
            0.06930693069306931,
            0.06930693069306931,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.0594059405940594,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.04950495049504951,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.039603960396039604,
            0.0297029702970297,
            0.0297029702970297,
            0.0297029702970297,
            0.0297029702970297,
            0.019801980198019802,
            0.019801980198019802,
            0.0,
        ]
        cls.expected_precision_lightgbm = [
            0.8049,
            0.8182,
            0.8462,
            0.8534,
            0.8684,
            0.8761,
            0.8761,
            0.8839,
            0.8919,
            0.8919,
            0.9083,
            0.9083,
            0.9074,
            0.9074,
            0.9159,
            0.9159,
            0.9159,
            0.9159,
            0.9159,
            0.9159,
            0.9159,
            0.9159,
            0.9159,
            0.9159,
            0.9151,
            0.9151,
            0.9151,
            0.9151,
            0.9151,
            0.9151,
            0.9151,
            0.9238,
            0.9238,
            0.9327,
            0.9327,
            0.932,
            0.9314,
            0.9307,
            0.9307,
            0.9307,
            0.9307,
            0.9307,
            0.94,
            0.94,
            0.94,
            0.94,
            0.94,
            0.94,
            0.94,
            0.94,
            0.94,
            0.94,
            0.94,
            0.94,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.949,
            0.9588,
            0.9588,
            0.9588,
            0.9583,
            0.9583,
            0.9583,
            0.9579,
            0.9579,
            0.9579,
            0.9579,
            0.9579,
            0.9574,
            0.9574,
            0.9574,
            0.9574,
            0.9574,
            0.957,
            0.9565,
            0.9565,
            0.956,
            0.956,
            0.956,
            0.956,
            0.9556,
            0.9663,
            0.9663,
            0.9655,
            0.9651,
            0.975,
            0.9726,
            0,
        ]
        cls.expected_recall_lightgbm = [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9899,
            0.9798,
            0.9798,
            0.9798,
            0.9798,
            0.9798,
            0.9798,
            0.9798,
            0.9798,
            0.9798,
            0.9798,
            0.9798,
            0.9697,
            0.9596,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9495,
            0.9394,
            0.9394,
            0.9394,
            0.9394,
            0.9293,
            0.9293,
            0.9293,
            0.9192,
            0.9192,
            0.9192,
            0.9192,
            0.9192,
            0.9091,
            0.9091,
            0.9091,
            0.9091,
            0.9091,
            0.899,
            0.8889,
            0.8889,
            0.8788,
            0.8788,
            0.8788,
            0.8788,
            0.8687,
            0.8687,
            0.8687,
            0.8485,
            0.8384,
            0.7879,
            0.7172,
            0.0,
        ]
        cls.expected_threshold_pr_lightgbm = 0.35
        cls.expected_threshold_roc_curve_lightgbm = 0.68

        # Feature importance
        cls.expected_feature_importance_logistic = pd.Series(
            {
                "feature_3": 0.878184846,
                "feature_0": 0.6807735248,
                "feature_8": 0.0515334867,
                "feature_2": 0.0358143277,
                "feature_9": -0.0381204653,
                "feature_6": -0.0883332814,
                "feature_7": -0.1301885331,
                "feature_1": -0.275092971,
                "feature_4": -0.6430405339,
                "feature_5": -0.7505753324,
            }
        )
        cls.expected_feature_importance_catboost = pd.Series(
            {
                "feature_4": 44.3253726823,
                "feature_5": 20.1107110351,
                "feature_0": 15.1218134278,
                "feature_3": 10.9642748095,
                "feature_9": 3.3279302184,
                "feature_1": 2.6251848816,
                "feature_6": 2.178141271,
                "feature_7": 0.8941374171,
                "feature_8": 0.2557275556,
                "feature_2": 0.1967067016,
            }
        )
        cls.expected_feature_importance_lightgbm = pd.Series(
            {
                "feature_0": 544,
                "feature_4": 515,
                "feature_5": 412,
                "feature_9": 317,
                "feature_3": 288,
                "feature_1": 215,
                "feature_7": 186,
                "feature_6": 185,
                "feature_2": 178,
                "feature_8": 142,
            }
        )

        cls.expected_feature_importance_catboost_regression = pd.Series(
            {
                "feature_1": 28.8642418698,
                "feature_5": 26.0319957349,
                "feature_9": 21.1700903921,
                "feature_2": 8.0373088615,
                "feature_0": 6.5587814183,
                "feature_6": 5.7264275476,
                "feature_7": 1.4749623714,
                "feature_3": 1.0968849531,
                "feature_8": 0.8971516205,
                "feature_4": 0.1421552306,
            }
        )
        cls.expected_metrics_catboost_regression = {
            "r2_score": 0.908,
            "mean_absolute_error": 30.17,
            "mean_squared_error": 1602.8024,
            "root_mean_squared_error": 40.035,
            "median_absolute_error": 24.5409,
        }

    def setUp(self):
        os.makedirs(self.folder_name, exist_ok=True)

    def tearDown(self):
        """
        Delete the folders created for the tests.
        """
        if os.path.exists(self.folder_name):
            shutil.rmtree(self.folder_name)

    def _check_model_with_zip(self, output_folder):
        """
        Helper function to check the model is saved correctly when zip_files is True.
        """
        # Assert zip files are saved
        self.assertTrue(os.path.exists(os.path.join(output_folder, "model.zip")))
        self.assertTrue(os.path.exists(os.path.join(output_folder, "datasets.zip")))

        # Assert files are saved correctly
        self.assertTrue(os.path.exists(os.path.join(output_folder, "summary.json")))

        # Assert files are not saved
        self.assertFalse(
            os.path.exists(os.path.join(output_folder, "model", "model.pkl"))
        )
        self.assertFalse(
            os.path.exists(os.path.join(output_folder, "model", "hyperparameters.json"))
        )
        self.assertFalse(
            os.path.exists(os.path.join(output_folder, "datasets", "X_train.csv"))
        )
        self.assertFalse(
            os.path.exists(os.path.join(output_folder, "datasets", "y_train.csv"))
        )
        self.assertFalse(
            os.path.exists(os.path.join(output_folder, "datasets", "X_test.csv"))
        )
        self.assertFalse(
            os.path.exists(os.path.join(output_folder, "datasets", "y_test.csv"))
        )

        # Assert subfolder not saved
        self.assertFalse(os.path.exists(os.path.join(output_folder, "model")))
        self.assertFalse(os.path.exists(os.path.join(output_folder, "datasets")))

    def _check_model_without_zip(self, model, output_folder, problem_type):
        """
        Helper function to check the model is saved correctly when zip_files is False.
        """
        # Assert folders are saved correctly
        self.assertTrue(os.path.exists(os.path.join(output_folder, "model")))
        self.assertTrue(os.path.exists(os.path.join(output_folder, "datasets")))
        # Assert files are saved correctly
        self.assertTrue(os.path.exists(os.path.join(output_folder, "summary.json")))
        self.assertTrue(
            os.path.exists(os.path.join(output_folder, "model", "hyperparameters.json"))
        )
        self.assertTrue(
            os.path.exists(os.path.join(output_folder, "model", "model.pkl"))
        )
        self.assertTrue(
            os.path.exists(os.path.join(output_folder, "datasets", "X_train.csv"))
        )
        self.assertTrue(
            os.path.exists(os.path.join(output_folder, "datasets", "y_train.csv"))
        )
        self.assertTrue(
            os.path.exists(os.path.join(output_folder, "datasets", "X_test.csv"))
        )
        self.assertTrue(
            os.path.exists(os.path.join(output_folder, "datasets", "y_test.csv"))
        )
        # Assert zip files are not saved
        self.assertFalse(os.path.exists(os.path.join(output_folder, "model.zip")))
        self.assertFalse(os.path.exists(os.path.join(output_folder, "datasets.zip")))
        # Assert files are valid for data folder
        X_train = pd.read_csv(os.path.join(output_folder, "datasets", "X_train.csv"))
        y_train = pd.read_csv(
            os.path.join(output_folder, "datasets", "y_train.csv")
        ).values
        X_test = pd.read_csv(os.path.join(output_folder, "datasets", "X_test.csv"))
        y_test = pd.read_csv(
            os.path.join(output_folder, "datasets", "y_test.csv")
        ).values
        if problem_type == ProblemType.CLASSIFICATION:
            assert_frame_equal(X_train, self.X_train_clf)
            self.assertListEqual(
                list([y for y in y_train.reshape(-1)]),
                list([y for y in self.y_train_clf.values]),
            )
            assert_frame_equal(X_test, self.X_test_clf)
            self.assertListEqual(
                list([y for y in y_test.reshape(-1)]),
                list([y for y in self.y_test_clf.values]),
            )
        elif problem_type == ProblemType.REGRESSION:
            assert_frame_equal(X_train, self.X_train_reg)
            self.assertListEqual(
                list([round(y, 4) for y in y_train.reshape(-1)]),
                list([round(y, 4) for y in self.y_train_reg.values]),
            )
            assert_frame_equal(X_test, self.X_test_reg)
            self.assertListEqual(
                list([round(y, 4) for y in y_test.reshape(-1)]),
                list([round(y, 4) for y in self.y_test_reg.values]),
            )
        else:
            raise ValueError("Problem type not supported")
        # Assert model is the same
        # Assert model is the same
        with open(os.path.join(output_folder, "model", "model.pkl"), "rb") as f:
            model_load = pickle.load(f)

        # Generate predictions from both models
        original_predictions = model.predict(self.X_test_reg)
        loaded_predictions = model_load.predict(self.X_test_reg)

        # Check if the predictions are almost the same
        self.assertTrue(np.allclose(original_predictions, loaded_predictions))

    def test_serialize_sklearn(self):
        """
        Test serialization of a sklearn model.
        """
        model = LinearRegression()
        model.fit(self.X_train_reg, self.y_train_reg)
        output_folder = export_model(
            model,
            self.X_train_reg,
            self.y_train_reg,
            self.X_test_reg,
            self.y_test_reg,
            self.folder_name,
            save_model=True,
            save_datasets=True,
            zip_files=False,
        )
        self._check_model_without_zip(
            output_folder=output_folder,
            model=model,
            problem_type=ProblemType.REGRESSION,
        )
        # Assert works for classification with Zip
        model = LogisticRegression()
        model.fit(self.X_train_clf, self.y_train_clf)
        output_folder = export_model(
            model,
            self.X_train_clf,
            self.y_train_clf,
            self.X_test_clf,
            self.y_test_clf,
            self.folder_name,
            save_model=True,
            save_datasets=True,
            zip_files=True,
        )
        self._check_model_with_zip(output_folder=output_folder)

    def test_serialize_catboost(self):
        """
        Test serialization of a CatBoost model.
        """
        model = CatBoostClassifier(allow_writing_files=False, verbose=5, iterations=10)
        model.fit(self.X_train_clf, self.y_train_clf)
        output_folder = export_model(
            model,
            self.X_train_clf,
            self.y_train_clf,
            self.X_test_clf,
            self.y_test_clf,
            self.folder_name,
            save_model=True,
            save_datasets=True,
            zip_files=False,
        )
        self._check_model_without_zip(
            output_folder=output_folder,
            model=model,
            problem_type=ProblemType.CLASSIFICATION,
        )

        # Assert works for regression with Zip
        model = CatBoostRegressor(allow_writing_files=False, verbose=5, iterations=10)
        model.fit(self.X_train_reg, self.y_train_reg)
        output_folder = export_model(
            model,
            self.X_train_reg,
            self.y_train_reg,
            self.X_test_reg,
            self.y_test_reg,
            self.folder_name,
            save_model=True,
            save_datasets=True,
            zip_files=True,
        )
        self._check_model_with_zip(output_folder=output_folder)

    def test_serialize_pipeline_with_catboost(self):
        """
        Test serialization of a pipeline with CatBoost model.
        """
        col_transformer = ColumnTransformer(
            [
                ("num", StandardScaler(), self.X_train_clf.columns),
            ]
        )
        model = Pipeline(
            [
                ("col_transformer", col_transformer),
                (
                    "model",
                    CatBoostClassifier(
                        allow_writing_files=False, verbose=5, iterations=10
                    ),
                ),
            ]
        )
        model.fit(self.X_train_clf, self.y_train_clf)
        output_folder = export_model(
            model,
            self.X_train_clf,
            self.y_train_clf,
            self.X_test_clf,
            self.y_test_clf,
            self.folder_name,
            save_model=True,
            save_datasets=True,
            zip_files=False,
        )
        self._check_model_without_zip(
            output_folder=output_folder,
            model=model,
            problem_type=ProblemType.CLASSIFICATION,
        )

    def test_serialize_lightgbm(self):
        """
        Test serialization of a LightGBM model.
        """
        model = LGBMClassifier()
        model.fit(self.X_train_clf, self.y_train_clf)
        output_folder = export_model(
            model,
            self.X_train_clf,
            self.y_train_clf,
            self.X_test_clf,
            self.y_test_clf,
            self.folder_name,
            save_model=True,
            save_datasets=True,
            zip_files=False,
        )
        self._check_model_without_zip(
            output_folder=output_folder,
            model=model,
            problem_type=ProblemType.CLASSIFICATION,
        )

        # Assert works for regression with Zip
        model = LGBMRegressor()
        model.fit(self.X_train_reg, self.y_train_reg)
        output_folder = export_model(
            model,
            self.X_train_reg,
            self.y_train_reg,
            self.X_test_reg,
            self.y_test_reg,
            self.folder_name,
            save_model=True,
            save_datasets=True,
            zip_files=True,
        )
        self._check_model_with_zip(output_folder=output_folder)

    def test_errors(self):
        """
        Test errors raised by the function.
        """
        # Not supported model
        model = InvalidModel()
        with self.assertRaises(ValueError):
            export_model(
                model,
                self.X_train_reg,
                self.y_train_reg,
                self.X_test_reg,
                self.y_test_reg,
                self.folder_name,
                save_model=True,
                save_datasets=True,
                zip_files=False,
            )

        # Invalid folder
        with self.assertRaises(FileNotFoundError):
            export_model(
                model,
                self.X_train_reg,
                self.y_train_reg,
                self.X_test_reg,
                self.y_test_reg,
                "invalid_folder",
                save_model=True,
                save_datasets=True,
                zip_files=False,
            )

    def assert_ml_experiment_init_correct(
        self,
        experiment,
        full_model,
        X_train,
        y_train,
        X_test,
        y_test,
        name,
        description,
        problem_type,
        base_library,
        base_model,
        num_classes=None,
        config=None,
    ):
        self.assertEqual(experiment.model, full_model)
        assert_frame_equal(experiment.X_train, X_train)
        assert_frame_equal(experiment.X_test, X_test)
        assert_series_equal(experiment.y_train, y_train)
        assert_series_equal(experiment.y_test, y_test)
        self.assertEqual(experiment.problem_type, problem_type)
        self.assertEqual(experiment.name, name)
        self.assertEqual(experiment.description, description)
        self.assertEqual(experiment.base_model, base_model)
        self.assertEqual(experiment.base_model_library, base_library)
        self.assertEqual(experiment.num_classes, num_classes)
        self.assertEqual(experiment._config, config)

    def test_ml_experiment_errors(self):
        # Create Logistic Regression model
        model = LogisticRegression()
        model.fit(self.X_train_clf, self.y_train_clf)
        # Create experiment
        experiment = MLExperiment(
            model=model,
            X_train=self.X_train_clf,
            y_train=self.y_train_clf,
            X_test=self.X_test_clf,
            y_test=self.y_test_clf,
            problem_type="classification",
            name="Test sklearn experiment",
            description="Test sklearn experiment",
        )
        # Assert experiment is created correctly
        self.assert_ml_experiment_init_correct(
            experiment=experiment,
            full_model=model,
            X_train=self.X_train_clf,
            y_train=self.y_train_clf,
            X_test=self.X_test_clf,
            y_test=self.y_test_clf,
            name="Test sklearn experiment",
            description="Test sklearn experiment",
            problem_type=ProblemType.CLASSIFICATION,
            base_library=ModelLibrary.SCIKIT_LEARN,
            base_model=model,  # Not a pipeline, hence, same as full model
            num_classes=3,
        )
        # Not implemented for n_classes > 2
        self.assertRaises(NotImplementedError, experiment.plot_roc_curve, show=False)
        self.assertRaises(NotImplementedError, experiment._calc_roc_curve_data)

    def _check_threshold_calculation(
        self,
        experiment,
        expected_tpr,
        expected_fpr,
        expected_precision,
        expected_recall,
        expected_threshold_pr,
        expected_threshold_roc_curve,
    ):
        """
        Test the threshold calculation for the roc curve or pr
        :param experiment:
        :return:
        """
        self.assertEqual(experiment._tpr_list, expected_tpr)
        self.assertEqual(experiment._fpr_list, expected_fpr)
        self.assertEqual(experiment._precision_list, expected_precision)
        self.assertEqual(experiment._recall_list, expected_recall)
        self.assertEqual(experiment.best_threshold_pr_curve, expected_threshold_pr)
        self.assertEqual(
            experiment.best_threshold_roc_curve, expected_threshold_roc_curve
        )

    def _check_feature_importance(self, experiment, expected_feature_importance):
        feature_importance = experiment.get_feature_importance()
        assert_series_equal(
            feature_importance, expected_feature_importance, check_dtype=False
        )

    def test_ml_experiment_sklearn(self):
        # Create Logistic Regression model
        model = LogisticRegression(random_state=33)
        model.fit(self.X_train_bin_clf, self.y_train_bin_clf)
        # Create experiment
        experiment = MLExperiment(
            model=model,
            X_train=self.X_train_bin_clf,
            y_train=self.y_train_bin_clf,
            X_test=self.X_test_bin_clf,
            y_test=self.y_test_bin_clf,
            problem_type="classification",
            name="Test sklearn experiment",
            description="Test sklearn experiment",
        )
        # Assert experiment is created correctly
        self.assert_ml_experiment_init_correct(
            experiment=experiment,
            full_model=model,
            X_train=self.X_train_bin_clf,
            y_train=self.y_train_bin_clf,
            X_test=self.X_test_bin_clf,
            y_test=self.y_test_bin_clf,
            name="Test sklearn experiment",
            description="Test sklearn experiment",
            problem_type=ProblemType.CLASSIFICATION,
            base_library=ModelLibrary.SCIKIT_LEARN,
            base_model=model,  # Not a pipeline, hence, same as full model
            num_classes=2,
        )

        # Roc curve
        self._check_threshold_calculation(
            experiment=experiment,
            expected_tpr=self.expected_tpr_logistic,
            expected_fpr=self.expected_fpr_logistic,
            expected_precision=self.expected_precision_logistic,
            expected_recall=self.expected_recall_logistic,
            expected_threshold_pr=self.expected_threshold_pr_logistic,
            expected_threshold_roc_curve=self.expected_threshold_roc_curve_logistic,
        )

        # Feature importance
        self._check_feature_importance(
            experiment,
            expected_feature_importance=self.expected_feature_importance_logistic,
        )

    def test_ml_experiment_catboost(self):
        # Create CatBoost model
        model = CatBoostClassifier(
            random_state=33, verbose=5, iterations=10, allow_writing_files=False
        )
        model.fit(self.X_train_bin_clf, self.y_train_bin_clf)
        # Create experiment
        experiment = MLExperiment(
            model=model,
            X_train=self.X_train_bin_clf,
            y_train=self.y_train_bin_clf,
            X_test=self.X_test_bin_clf,
            y_test=self.y_test_bin_clf,
            problem_type="classification",
            name="Test catboost experiment",
            description="Test catboost experiment",
        )
        # Assert experiment is created correctly
        self.assert_ml_experiment_init_correct(
            experiment=experiment,
            full_model=model,
            X_train=self.X_train_bin_clf,
            y_train=self.y_train_bin_clf,
            X_test=self.X_test_bin_clf,
            y_test=self.y_test_bin_clf,
            name="Test catboost experiment",
            description="Test catboost experiment",
            problem_type=ProblemType.CLASSIFICATION,
            base_library=ModelLibrary.CATBOOST,
            base_model=model,  # Not a pipeline, hence, same as full model
            num_classes=2,
        )

        # Roc curve
        self._check_threshold_calculation(
            experiment,
            expected_tpr=self.expected_tpr_catboost,
            expected_fpr=self.expected_fpr_catboost,
            expected_precision=self.expected_precision_catboost,
            expected_recall=self.expected_recall_catboost,
            expected_threshold_pr=self.expected_threshold_pr_catboost,
            expected_threshold_roc_curve=self.expected_threshold_roc_curve_catboost,
        )

        # Feature importance
        self._check_feature_importance(
            experiment,
            expected_feature_importance=self.expected_feature_importance_catboost,
        )

        # Test with regression
        # Create CatBoost model
        model = CatBoostRegressor(
            random_state=33, verbose=5, iterations=10, allow_writing_files=False
        )
        model.fit(self.X_train_reg, self.y_train_reg)
        # Create experiment
        experiment = MLExperiment(
            model=model,
            X_train=self.X_train_reg,
            y_train=self.y_train_reg,
            X_test=self.X_test_reg,
            y_test=self.y_test_reg,
            problem_type="regression",
            name="Test catboost experiment",
            description="Test catboost experiment",
        )
        # Assert experiment is created correctly
        self.assert_ml_experiment_init_correct(
            experiment=experiment,
            full_model=model,
            X_train=self.X_train_reg,
            y_train=self.y_train_reg,
            X_test=self.X_test_reg,
            y_test=self.y_test_reg,
            name="Test catboost experiment",
            description="Test catboost experiment",
            problem_type=ProblemType.REGRESSION,
            base_library=ModelLibrary.CATBOOST,
            base_model=model,  # Not a pipeline, hence, same as full model
        )

        # Feature importance
        self._check_feature_importance(
            experiment,
            expected_feature_importance=self.expected_feature_importance_catboost_regression,
        )

        # Metrics
        self.assertDictEqual(
            experiment.metrics,
            self.expected_metrics_catboost_regression,
        )

    def test_ml_experiment_pipeline_with_catboost(self):
        # Create CatBoost model
        model = CatBoostClassifier(
            random_state=33, verbose=5, iterations=10, allow_writing_files=False
        )
        col_transformer = ColumnTransformer(
            [
                ("num", "passthrough", self.X_train_bin_clf.columns),
            ],
            verbose_feature_names_out=False,
        )
        # Create pipeline
        pipeline = Pipeline(
            [
                ("col_transformer", col_transformer),
                ("model", model),
            ]
        )
        pipeline.fit(self.X_train_bin_clf, self.y_train_bin_clf)
        # Create experiment
        experiment = MLExperiment(
            model=pipeline,
            X_train=self.X_train_bin_clf,
            y_train=self.y_train_bin_clf,
            X_test=self.X_test_bin_clf,
            y_test=self.y_test_bin_clf,
            problem_type="classification",
            name="Test catboost experiment",
            description="Test catboost experiment",
        )
        # Assert experiment is created correctly
        self.assert_ml_experiment_init_correct(
            experiment=experiment,
            full_model=pipeline,
            X_train=self.X_train_bin_clf,
            y_train=self.y_train_bin_clf,
            X_test=self.X_test_bin_clf,
            y_test=self.y_test_bin_clf,
            name="Test catboost experiment",
            description="Test catboost experiment",
            problem_type=ProblemType.CLASSIFICATION,
            base_library=ModelLibrary.CATBOOST,
            base_model=model,
            num_classes=2,
        )

        # Roc curve
        self._check_threshold_calculation(
            experiment,
            expected_tpr=self.expected_tpr_catboost,
            expected_fpr=self.expected_fpr_catboost,
            expected_precision=self.expected_precision_catboost,
            expected_recall=self.expected_recall_catboost,
            expected_threshold_pr=self.expected_threshold_pr_catboost,
            expected_threshold_roc_curve=self.expected_threshold_roc_curve_catboost,
        )

        # Feature importance
        self._check_feature_importance(
            experiment,
            expected_feature_importance=self.expected_feature_importance_catboost,
        )

    def test_ml_experiment_lightgbm(self):
        # Create LightGBM model
        model = LGBMClassifier(random_state=33)
        model.fit(self.X_train_bin_clf, self.y_train_bin_clf)
        # Create experiment
        experiment = MLExperiment(
            model=model,
            X_train=self.X_train_bin_clf,
            y_train=self.y_train_bin_clf,
            X_test=self.X_test_bin_clf,
            y_test=self.y_test_bin_clf,
            problem_type="classification",
            name="Test lightgbm experiment",
            description="Test lightgbm experiment",
        )
        # Assert experiment is created correctly
        self.assert_ml_experiment_init_correct(
            experiment=experiment,
            full_model=model,
            X_train=self.X_train_bin_clf,
            y_train=self.y_train_bin_clf,
            X_test=self.X_test_bin_clf,
            y_test=self.y_test_bin_clf,
            name="Test lightgbm experiment",
            description="Test lightgbm experiment",
            problem_type=ProblemType.CLASSIFICATION,
            base_library=ModelLibrary.LIGHTGBM,
            base_model=model,  # Not a pipeline, hence, same as full model
            num_classes=2,
        )

        # Roc curve
        self._check_threshold_calculation(
            experiment,
            expected_tpr=self.expected_tpr_lightgbm,
            expected_fpr=self.expected_fpr_lightgbm,
            expected_precision=self.expected_precision_lightgbm,
            expected_recall=self.expected_recall_lightgbm,
            expected_threshold_pr=self.expected_threshold_pr_lightgbm,
            expected_threshold_roc_curve=self.expected_threshold_roc_curve_lightgbm,
        )

        # Feature importance
        self._check_feature_importance(
            experiment,
            expected_feature_importance=self.expected_feature_importance_lightgbm,
        )

    def test_ml_tracker_add_experiment(self):
        ml_tracker = MLTracker(experiment_folder=self.folder_name)

        # Create a couple of experiments for the binary classification problem
        # Create Logistic Regression model
        model = LogisticRegression(random_state=33)
        model.fit(self.X_train_bin_clf, self.y_train_bin_clf)

        # Create experiment
        experiment = MLExperiment(
            model=model,
            X_train=self.X_train_bin_clf,
            y_train=self.y_train_bin_clf,
            X_test=self.X_test_bin_clf,
            y_test=self.y_test_bin_clf,
            problem_type="classification",
            name="Test sklearn experiment",
            description="Test sklearn experiment",
        )

        ml_tracker.add_experiment(experiment)

        # Assert experiment in tracker
        self.assertEqual(len(ml_tracker.experiments), 1)

        # Test scan for experiments
        ml_tracker_new = MLTracker(experiment_folder=self.folder_name)

        ml_tracker_new.scan_for_experiments()

        self.assertEqual(len(ml_tracker_new.experiments), 1)

        # Assert experiments are equal
        self.assertEqual(list(ml_tracker_new.experiments.values())[0], experiment)
